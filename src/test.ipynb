{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "151c51b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.insert(0, os.path.abspath('..'))\n",
                "from src.dataset_pipeline import load_dataset\n",
                "import numpy as np\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from sklearn.metrics import normalized_mutual_info_score, homogeneity_score, silhouette_score\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "# Assuming these are your custom modules based on the paper's architecture\n",
                "from src.clust_dr import DR_then_Clust\n",
                "from src.affinities import NormalizedGaussianAndStudentAffinity, SymmetricEntropicAffinity\n",
                "from src.clust_dr import Clust_then_DR, DistR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "d699eb8a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading mnist...\n",
                        "Loading mnist...\n",
                        "Original shape: (10000, 784)\n",
                        "Applying PCA to 50 dimensions...\n",
                        "Successfully loaded mnist\n",
                        "  X shape: (10000, 50)\n",
                        "  Y shape: (10000,)\n",
                        "  Original dim: 784\n",
                        "------------------------------\n",
                        "Loading fmnist...\n",
                        "Loading fmnist...\n",
                        "Original shape: (10000, 784)\n",
                        "Applying PCA to 50 dimensions...\n",
                        "Successfully loaded fmnist\n",
                        "  X shape: (10000, 50)\n",
                        "  Y shape: (10000,)\n",
                        "  Original dim: 784\n",
                        "------------------------------\n",
                        "Loading pbmc...\n",
                        "Loading pbmc...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/python/lib/python3.13/site-packages/scanpy/preprocessing/_simple.py:176: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
                        "  adata.obs[\"n_genes\"] = number\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original shape: (2638, 2000)\n",
                        "Applying PCA to 50 dimensions...\n",
                        "Successfully loaded pbmc\n",
                        "  X shape: (2638, 50)\n",
                        "  Y shape: (2638,)\n",
                        "  Original dim: 2000\n",
                        "------------------------------\n",
                        "Loading zeisel...\n",
                        "Loading zeisel...\n",
                        "Failed to load zeisel: scanpy.datasets.zeisel() not found. Please update scanpy.\n",
                        "------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# List of datasets to load\n",
                "# Excluded 'coil20' due to current server downtime (404/522)\n",
                "# 'snareseq' requires manual download of multiple files usually, but we can try if implemented.\n",
                "# We focus on the ones known to work or standard in scanpy/torchvision.\n",
                "datasets_to_load = ['mnist', 'fmnist', 'pbmc', 'zeisel']\n",
                "\n",
                "loaded_data = {}\n",
                "\n",
                "for name in datasets_to_load:\n",
                "    try:\n",
                "        print(f\"Loading {name}...\")\n",
                "        # Load with default PCA dim 50\n",
                "        data = load_dataset(name, pca_dim=50)\n",
                "        loaded_data[name] = data\n",
                "        \n",
                "        print(f\"Successfully loaded {name}\")\n",
                "        print(f\"  X shape: {data['X'].shape}\")\n",
                "        print(f\"  Y shape: {data['Y'].shape}\")\n",
                "        print(f\"  Original dim: {data['original_dim']}\")\n",
                "        print(\"-\" * 30)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to load {name}: {e}\")\n",
                "        print(\"-\" * 30)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "e778c9d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MNIST data ready for DistR.\n"
                    ]
                }
            ],
            "source": [
                "# Example access\n",
                "if 'mnist' in loaded_data:\n",
                "    X_mnist = loaded_data['mnist']['X']\n",
                "    Y_mnist = loaded_data['mnist']['Y']\n",
                "    print(\"MNIST data ready for DistR.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06b0190b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Master Process Device: cuda\n",
                        "GPU: Tesla T4\n",
                        "=== Running Experiment on mnist ===\n",
                        "Configuration: N=10000, Prototypes=30, Device=cuda\n",
                        "\n",
                        "[1/3] Training DistR (Device: cuda)...\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "def get_majority_vote_labels(T, Y):\n",
                "    \"\"\"\n",
                "    Assigns a class label to each prototype based on the majority class \n",
                "    of the samples mapped to it (weighted by the transport plan T).\n",
                "    Operations are fully vectorized on GPU.\n",
                "    \"\"\"\n",
                "    # T: (N_samples, n_prototypes) [GPU]\n",
                "    # Y: (N_samples,) [CPU/Numpy]\n",
                "    \n",
                "    n_prototypes = T.shape[1]\n",
                "    # Convert Y to GPU tensor once\n",
                "    Y_tensor = torch.as_tensor(Y, device=T.device, dtype=torch.long)\n",
                "    n_classes = Y_tensor.max().item() + 1\n",
                "    \n",
                "    # One-hot encode Y to aggregate mass: (N_samples, n_classes)\n",
                "    # We use scatter on GPU\n",
                "    Y_onehot = torch.zeros((len(Y), n_classes), device=T.device, dtype=T.dtype)\n",
                "    Y_onehot.scatter_(1, Y_tensor.unsqueeze(1), 1.0)\n",
                "    \n",
                "    # Aggregate mass: Project Y onto Prototypes via T^T\n",
                "    # vote_matrix: (n_prototypes, n_classes)\n",
                "    # This matrix multiplication is the heavy lifting, kept on GPU\n",
                "    vote_matrix = torch.mm(T.T, Y_onehot)\n",
                "    \n",
                "    # Get majority class for each prototype\n",
                "    proto_labels = torch.argmax(vote_matrix, dim=1).cpu().numpy()\n",
                "    return proto_labels\n",
                "\n",
                "def evaluate_prototypes(Z, T, Y_true):\n",
                "    \"\"\"\n",
                "    Computes metrics defined in the paper [cite: 332-339].\n",
                "    Transfers to CPU only when necessary for sklearn.\n",
                "    \"\"\"\n",
                "    # Z and T are on GPU\n",
                "    \n",
                "    # 1. Homogeneity Score (Evaluated on Samples)\n",
                "    # Hard assignment: x_i assigned to prototype j with max T_ij\n",
                "    # Perform argmax on GPU before transfer\n",
                "    sample_assignments = torch.argmax(T, dim=1).cpu().numpy()\n",
                "    homogeneity = homogeneity_score(Y_true, sample_assignments)\n",
                "    \n",
                "    # Data transfer for K-Means and Silhouette (sklearn is CPU-bound)\n",
                "    Z_np = Z.detach().cpu().numpy()\n",
                "    \n",
                "    # 2. K-Means NMI Score (Evaluated on Prototypes)\n",
                "    n_classes = len(np.unique(Y_true))\n",
                "    # Note: If you have cuML installed, KMeans can be run on GPU here.\n",
                "    kmeans = KMeans(n_clusters=n_classes, random_state=42, n_init=10)\n",
                "    proto_clusters = kmeans.fit_predict(Z_np)\n",
                "    \n",
                "    # Propagate prototype clusters back to samples\n",
                "    predicted_labels = proto_clusters[sample_assignments]\n",
                "    nmi = normalized_mutual_info_score(Y_true, predicted_labels)\n",
                "    \n",
                "    # 3. Silhouette Score (Weighted by Prototype Mass)\n",
                "    proto_labels = get_majority_vote_labels(T, Y_true)\n",
                "    \n",
                "    # Calculate weights (mass of each prototype) on GPU\n",
                "    weights = T.sum(dim=0).cpu().numpy()\n",
                "    \n",
                "    try:\n",
                "        # Silhouette requires CPU\n",
                "        sil = silhouette_score(Z_np, proto_labels, sample_weight=weights)\n",
                "    except ValueError:\n",
                "        sil = -1.0\n",
                "        \n",
                "    return {\n",
                "        \"Homogeneity\": homogeneity,\n",
                "        \"NMI\": nmi,\n",
                "        \"Silhouette\": sil\n",
                "    }\n",
                "\n",
                "def run_experiment(data_dict, dataset_name, n_prototypes=50, device='cuda'):\n",
                "    \"\"\"\n",
                "    Runs DistR and Baselines on a single dataset.\n",
                "    \"\"\"\n",
                "    print(f\"=== Running Experiment on {dataset_name} ===\")\n",
                "    print(f\"Configuration: N={data_dict['X'].shape[0]}, Prototypes={n_prototypes}, Device={device}\")\n",
                "    \n",
                "    # OPTIMIZATION: Use float32 for faster GPU computation (Tensor Cores)\n",
                "    # Create tensor directly on device to avoid CPU->GPU copy\n",
                "    X = torch.as_tensor(data_dict['X'], dtype=torch.float32, device=device)\n",
                "    Y = data_dict['Y'] \n",
                "    \n",
                "    # Initialize Affinities\n",
                "    # Ensure these classes respect the inputs' dtype and device!\n",
                "    # [cite: 329] Symmetric Entropic Affinity for X\n",
                "    affinity_X = SymmetricEntropicAffinity(perp=30, verbose=False) \n",
                "    # [cite: 329] Student t-distribution for Z\n",
                "    affinity_Z = NormalizedGaussianAndStudentAffinity(student=True) \n",
                "    \n",
                "    results = {}\n",
                "    \n",
                "    # 1. DistR (Ours)\n",
                "    print(f\"\\n[1/3] Training DistR (Device: {device})...\")\n",
                "    model_distr = DistR(\n",
                "        affinity_data=affinity_X,\n",
                "        affinity_embedding=affinity_Z,\n",
                "        output_sam=n_prototypes,\n",
                "        loss_fun=\"kl_loss\",       # [cite: 120]\n",
                "        optimizer=\"Adam\",         # [cite: 233]\n",
                "        lr=0.1,\n",
                "        max_iter=200,             # BCD Iterations [cite: 232]\n",
                "        device=device,\n",
                "        init=\"normal\",\n",
                "        init_T=\"kmeans\"\n",
                "    )\n",
                "    Z_distr = model_distr.fit_transform(X)\n",
                "    metrics_distr = evaluate_prototypes(Z_distr, model_distr.T, Y)\n",
                "    results['DistR'] = metrics_distr\n",
                "    print(f\"DistR Results: {metrics_distr}\")\n",
                "\n",
                "    # 2. DR -> Clustering\n",
                "    print(f\"\\n[2/3] Training DR -> Clustering (Device: {device})...\")\n",
                "    model_drc = DR_then_Clust(\n",
                "        affinity_data=affinity_X,\n",
                "        affinity_embedding=affinity_Z,\n",
                "        output_sam=n_prototypes,\n",
                "        output_dim=2,\n",
                "        loss_fun=\"kl_loss\",\n",
                "        device=device,\n",
                "        init=\"normal\",\n",
                "        init_T=\"kmeans\"\n",
                "    )\n",
                "    Z_drc = model_drc.fit_transform(X)\n",
                "    metrics_drc = evaluate_prototypes(Z_drc, model_drc.T, Y)\n",
                "    results['DR_then_Clust'] = metrics_drc\n",
                "    print(f\"DR->C Results: {metrics_drc}\")\n",
                "\n",
                "    # 3. Clustering -> DR\n",
                "    print(f\"\\n[3/3] Training Clustering -> DR (Device: {device})...\")\n",
                "    model_cdr = Clust_then_DR(\n",
                "        affinity_data=affinity_X,\n",
                "        affinity_embedding=affinity_Z,\n",
                "        output_sam=n_prototypes,\n",
                "        output_dim=2,\n",
                "        loss_fun=\"kl_loss\",\n",
                "        device=device,\n",
                "        init=\"normal\",\n",
                "        init_T=\"kmeans\"\n",
                "    )\n",
                "    Z_cdr = model_cdr.fit_transform(X)\n",
                "    metrics_cdr = evaluate_prototypes(Z_cdr, model_cdr.T, Y)\n",
                "    results['Clust_then_DR'] = metrics_cdr\n",
                "    print(f\"C->DR Results: {metrics_cdr}\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    # Auto-detect device\n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "    print(f\"Master Process Device: {device}\")\n",
                "    if device == 'cuda':\n",
                "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    \n",
                "    # Placeholder for loaded_data context\n",
                "    # loaded_data should be a dictionary: {'dataset_name': {'X': np.array, 'Y': np.array}}\n",
                "    target_dataset = 'mnist'\n",
                "    \n",
                "    if 'loaded_data' in locals() and target_dataset in loaded_data:\n",
                "        data_subset = {\n",
                "            'X': loaded_data[target_dataset]['X'],\n",
                "            'Y': loaded_data[target_dataset]['Y']\n",
                "        }\n",
                "        \n",
                "        # Strategy: n_prototypes = n_classes + 20 [cite: 1114]\n",
                "        n_classes = len(np.unique(data_subset['Y']))\n",
                "        n_prototypes = n_classes + 20\n",
                "        \n",
                "        final_scores = run_experiment(\n",
                "            data_subset, \n",
                "            target_dataset, \n",
                "            n_prototypes=n_prototypes, \n",
                "            device=device\n",
                "        )\n",
                "        \n",
                "        print(\"\\n=== Final Benchmark Report ===\")\n",
                "        for method, scores in final_scores.items():\n",
                "            print(f\"{method}:\")\n",
                "            for metric, val in scores.items():\n",
                "                print(f\"  {metric}: {val:.4f}\")\n",
                "    else:\n",
                "        print(f\"Warning: Dataset '{target_dataset}' data not found in local context. Script is ready for integration.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
