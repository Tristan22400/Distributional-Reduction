{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "151c51b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "sys.path.insert(0, os.path.abspath('..'))\n",
                "from src.dataset_pipeline import load_dataset\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d699eb8a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# List of datasets to load\n",
                "# Excluded 'coil20' due to current server downtime (404/522)\n",
                "# 'snareseq' requires manual download of multiple files usually, but we can try if implemented.\n",
                "# We focus on the ones known to work or standard in scanpy/torchvision.\n",
                "datasets_to_load = ['mnist', 'fmnist', 'pbmc', 'zeisel']\n",
                "\n",
                "loaded_data = {}\n",
                "\n",
                "for name in datasets_to_load:\n",
                "    try:\n",
                "        print(f\"Loading {name}...\")\n",
                "        # Load with default PCA dim 50\n",
                "        data = load_dataset(name, pca_dim=50)\n",
                "        loaded_data[name] = data\n",
                "        \n",
                "        print(f\"Successfully loaded {name}\")\n",
                "        print(f\"  X shape: {data['X'].shape}\")\n",
                "        print(f\"  Y shape: {data['Y'].shape}\")\n",
                "        print(f\"  Original dim: {data['original_dim']}\")\n",
                "        print(\"-\" * 30)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to load {name}: {e}\")\n",
                "        print(\"-\" * 30)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "e778c9d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MNIST data ready for DistR.\n"
                    ]
                }
            ],
            "source": [
                "# Example access\n",
                "if 'mnist' in loaded_data:\n",
                "    X_mnist = loaded_data['mnist']['X']\n",
                "    Y_mnist = loaded_data['mnist']['Y']\n",
                "    print(\"MNIST data ready for DistR.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06b0190b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "from sklearn.metrics import normalized_mutual_info_score, homogeneity_score, silhouette_score\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "from build.lib.src.clust_dr import DR_then_Clust\n",
                "from src.affinities import NormalizedGaussianAndStudentAffinity, SymmetricEntropicAffinity\n",
                "from src.clust_dr import Clust_then_DR, DistR\n",
                "\n",
                "# Assuming previous classes (DistR, DR_then_Clust, etc.) are in a module named 'methods'\n",
                "# from methods import DistR, DR_then_Clust, Clust_then_DR, COOTClust\n",
                "# from src.affinities import SymmetricEntropicAffinity, NormalizedGaussianAndStudentAffinity\n",
                "\n",
                "def get_majority_vote_labels(T, Y):\n",
                "    \"\"\"\n",
                "    Assigns a class label to each prototype based on the majority class \n",
                "    of the samples mapped to it (weighted by the transport plan T).\n",
                "    \"\"\"\n",
                "    # T is (N_samples, n_prototypes)\n",
                "    # Y is (N_samples,)\n",
                "    n_prototypes = T.shape[1]\n",
                "    n_classes = len(np.unique(Y))\n",
                "    \n",
                "    # Soft vote matrix: (n_prototypes, n_classes)\n",
                "    vote_matrix = torch.zeros((n_prototypes, n_classes), device=T.device)\n",
                "    \n",
                "    # One-hot encode Y to aggregate mass\n",
                "    Y_onehot = torch.zeros((len(Y), n_classes), device=T.device)\n",
                "    Y_onehot.scatter_(1, torch.tensor(Y, device=T.device).unsqueeze(1).long(), 1)\n",
                "    \n",
                "    # Aggregate mass: Project Y onto Prototypes via T^T\n",
                "    # vote_matrix[j, c] = sum of T_ij for all i where y_i = c\n",
                "    vote_matrix = T.T @ Y_onehot\n",
                "    \n",
                "    # Get majority class\n",
                "    proto_labels = torch.argmax(vote_matrix, dim=1).cpu().numpy()\n",
                "    return proto_labels\n",
                "\n",
                "def evaluate_prototypes(Z, T, Y_true):\n",
                "    \"\"\"\n",
                "    Computes the three metrics defined in the paper [cite: 332-339]:\n",
                "    1. Homogeneity: Do prototypes represent pure classes?\n",
                "    2. K-Means NMI: Does clustering the prototypes recover global structure?\n",
                "    3. Silhouette: Are prototypes well-separated?\n",
                "    \"\"\"\n",
                "    Z_np = Z.detach().cpu().numpy()\n",
                "    T_tensor = T.detach()\n",
                "    \n",
                "    # 1. Homogeneity Score\n",
                "    # Treat prototype assignment as a clustering of inputs\n",
                "    # Hard assignment: x_i assigned to prototype j with max T_ij\n",
                "    sample_assignments = torch.argmax(T_tensor, dim=1).cpu().numpy()\n",
                "    homogeneity = homogeneity_score(Y_true, sample_assignments)\n",
                "    \n",
                "    # 2. K-Means NMI Score\n",
                "    # Cluster the prototypes themselves into K=n_classes clusters\n",
                "    n_classes = len(np.unique(Y_true))\n",
                "    kmeans = KMeans(n_clusters=n_classes, random_state=42, n_init=10)\n",
                "    proto_clusters = kmeans.fit_predict(Z_np)\n",
                "    \n",
                "    # Propagate prototype clusters back to samples\n",
                "    predicted_labels = proto_clusters[sample_assignments]\n",
                "    nmi = normalized_mutual_info_score(Y_true, predicted_labels)\n",
                "    \n",
                "    # 3. Silhouette Score\n",
                "    # Computed on prototypes using majority vote labels\n",
                "    proto_labels = get_majority_vote_labels(T_tensor, Y_true)\n",
                "    # We weight samples by the mass of the prototype (sum of column T)\n",
                "    weights = T_tensor.sum(dim=0).cpu().numpy()\n",
                "    \n",
                "    try:\n",
                "        sil = silhouette_score(Z_np, proto_labels, sample_weight=weights)\n",
                "    except ValueError:\n",
                "        # Fails if < 2 labels present\n",
                "        sil = -1.0\n",
                "        \n",
                "    return {\n",
                "        \"Homogeneity\": homogeneity,\n",
                "        \"NMI\": nmi,\n",
                "        \"Silhouette\": sil\n",
                "    }\n",
                "\n",
                "def run_experiment(data_dict, dataset_name, n_prototypes=50, device='cuda'):\n",
                "    \"\"\"\n",
                "    Runs DistR and Baselines on a single dataset.\n",
                "    \"\"\"\n",
                "    print(f\"=== Running Experiment on {dataset_name} ===\")\n",
                "    print(f\"Configuration: N={data_dict['X'].shape[0]}, Prototypes={n_prototypes}, Device={device}\")\n",
                "    \n",
                "    X = torch.tensor(data_dict['X'], dtype=torch.float64).to(device)\n",
                "    Y = data_dict['Y'] # Keep as numpy/list for sklearn metrics\n",
                "    \n",
                "    # --- Configuration matching Paper Section 5  ---\n",
                "    # Input Affinity: Symmetric Entropic Affinity (SEA)\n",
                "    # Embedding Affinity: Student t-distribution\n",
                "    # Loss: KL Divergence\n",
                "    \n",
                "    # Note: Perplexity typically N/100 or 30. Fixed to 30 for consistency.\n",
                "    affinity_X = SymmetricEntropicAffinity(perp=30, verbose=False) \n",
                "    affinity_Z = NormalizedGaussianAndStudentAffinity(student=True) # Student kernel\n",
                "    \n",
                "    results = {}\n",
                "    \n",
                "    # 1. DistR (Ours)\n",
                "    print(\"\\n[1/3] Training DistR...\")\n",
                "    model_distr = DistR(\n",
                "        affinity_data=affinity_X,\n",
                "        affinity_embedding=affinity_Z,\n",
                "        output_sam=n_prototypes,\n",
                "        loss_fun=\"kl_loss\",       # Matching NE objective [cite: 120]\n",
                "        optimizer=\"Adam\",\n",
                "        lr=0.1,                   # Standard LR for Adam\n",
                "        max_iter=200,             # Sufficient for demo\n",
                "        device=device,\n",
                "        init=\"normal\",            # Must be \"normal\" or \"WrappedNormal\"\n",
                "        init_T=\"kmeans\"           # Good initialization\n",
                "    )\n",
                "    Z_distr = model_distr.fit_transform(X)\n",
                "    metrics_distr = evaluate_prototypes(Z_distr, model_distr.T, Y)\n",
                "    results['DistR'] = metrics_distr\n",
                "    print(f\"DistR Results: {metrics_distr}\")\n",
                "\n",
                "    # 2. DR -> Clustering\n",
                "    print(\"\\n[2/3] Training DR -> Clustering...\")\n",
                "    model_drc = DR_then_Clust(\n",
                "        affinity_data=affinity_X,\n",
                "        affinity_embedding=affinity_Z,\n",
                "        output_sam=n_prototypes,\n",
                "        output_dim=2,\n",
                "        loss_fun=\"kl_loss\",\n",
                "        device=device,\n",
                "        init=\"normal\",            # Must be \"normal\" or \"WrappedNormal\"\n",
                "        init_T=\"kmeans\"           # Uses kmeans to cluster the embeddings\n",
                "    )\n",
                "    Z_drc = model_drc.fit_transform(X)\n",
                "    metrics_drc = evaluate_prototypes(Z_drc, model_drc.T, Y)\n",
                "    results['DR_then_Clust'] = metrics_drc\n",
                "    print(f\"DR->C Results: {metrics_drc}\")\n",
                "\n",
                "    # 3. Clustering -> DR\n",
                "    print(\"\\n[3/3] Training Clustering -> DR...\")\n",
                "    model_cdr = Clust_then_DR(\n",
                "        affinity_data=affinity_X,\n",
                "        affinity_embedding=affinity_Z,\n",
                "        output_sam=n_prototypes,\n",
                "        output_dim=2,\n",
                "        loss_fun=\"kl_loss\",\n",
                "        device=device,\n",
                "        init=\"normal\",            # Must be \"normal\" or \"WrappedNormal\"\n",
                "        init_T=\"kmeans\"           # Uses kmeans to cluster input X\n",
                "    )\n",
                "    Z_cdr = model_cdr.fit_transform(X)\n",
                "    metrics_cdr = evaluate_prototypes(Z_cdr, model_cdr.T, Y)\n",
                "    results['Clust_then_DR'] = metrics_cdr\n",
                "    print(f\"C->DR Results: {metrics_cdr}\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "# === Execution Block ===\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    # Check for available device\n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "    \n",
                "    # Example Access as requested\n",
                "    target_dataset = 'mnist'\n",
                "    \n",
                "    if target_dataset in loaded_data:\n",
                "        # Prepare data dictionary from loaded context\n",
                "        data_subset = {\n",
                "            'X': loaded_data[target_dataset]['X'], # Ensure shape (N, Features)\n",
                "            'Y': loaded_data[target_dataset]['Y']\n",
                "        }\n",
                "        \n",
                "        # Set prototypes: typically n_classes + 20 or fixed number [cite: 1114]\n",
                "        n_classes = len(np.unique(data_subset['Y']))\n",
                "        n_prototypes = n_classes + 20 # Paper strategy\n",
                "        \n",
                "        final_scores = run_experiment(\n",
                "            data_subset, \n",
                "            target_dataset, \n",
                "            n_prototypes=n_prototypes, \n",
                "            device=device\n",
                "        )\n",
                "        \n",
                "        print(\"\\n=== Final Benchmark Report ===\")\n",
                "        for method, scores in final_scores.items():\n",
                "            print(f\"{method}:\")\n",
                "            for metric, val in scores.items():\n",
                "                print(f\"  {metric}: {val:.4f}\")\n",
                "    else:\n",
                "        print(f\"Dataset {target_dataset} not found in loaded_data.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "testpytorch",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
